"""
æ•¸æ“šçˆ¬èŸ²æ¨¡çµ„ - æ•´åˆ eps_dividend_scraper.pyã€fundamental_scraper.pyã€twse_scraper.py
å¢å¼·ç‰ˆæœ¬ï¼šæ”¹é€²æ•¸æ“šç²å–å¯é æ€§å’ŒéŒ¯èª¤è™•ç† (2025ç‰ˆ - ä¿®å¾©è¶…æ™‚è¨­ç½®)
"""
print("[scraper] âœ… å·²è¼‰å…¥æœ€æ–°ç‰ˆ")

import requests
import pandas as pd
from io import StringIO
import datetime
from bs4 import BeautifulSoup
import io
import time
import os
import json
import random
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import socket

# ç·©å­˜ç›®éŒ„è¨­ç½®
CACHE_DIR = os.path.join(os.path.dirname(__file__), '../../cache')
os.makedirs(CACHE_DIR, exist_ok=True)

# å…¨å±€é…ç½®åƒæ•¸ - å¾ç’°å¢ƒè®Šé‡ç²å–æˆ–ä½¿ç”¨é»˜èªå€¼
MAX_RETRIES = int(os.getenv("SCRAPER_MAX_RETRIES", "3"))
CONNECTION_TIMEOUT = int(os.getenv("SCRAPER_CONNECTION_TIMEOUT", "10"))
READ_TIMEOUT = int(os.getenv("SCRAPER_READ_TIMEOUT", "20"))
BATCH_DELAY = float(os.getenv("SCRAPER_BATCH_DELAY", "1.5"))

# å»ºç«‹ä¸€å€‹å¯é‡è©¦çš„ requests session
def create_retry_session(retries=MAX_RETRIES, backoff_factor=0.5, 
                         status_forcelist=(500, 502, 504, 429),
                         allowed_methods=('GET', 'POST')):
    """
    å»ºç«‹ä¸€å€‹å…·æœ‰é‡è©¦åŠŸèƒ½çš„ requests session
    """
    session = requests.Session()
    retry = Retry(
        total=retries,
        read=retries,
        connect=retries,
        backoff_factor=backoff_factor,
        status_forcelist=status_forcelist,
        allowed_methods=allowed_methods
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    
    # è¨­å®šæ›´é•·çš„é»˜èªè¶…æ™‚ - ä¿®å¾©ç‰ˆæœ¬ï¼Œé¿å…ä½¿ç”¨ super
    original_request = session.request
    session.request = lambda method, url, **kwargs: original_request(
        method=method, 
        url=url, 
        timeout=kwargs.pop('timeout', (CONNECTION_TIMEOUT, READ_TIMEOUT)),  # ä½¿ç”¨æä¾›çš„è¶…æ™‚æˆ–é»˜èªå€¼
        **kwargs
    )
    
    return session

def get_latest_season():
    """
    ç²å–æœ€è¿‘ä¸€å­£çš„å¹´åº¦å’Œå­£åº¦
    
    è¿”å›:
    - æ°‘åœ‹å¹´ï¼Œå­£åº¦(01, 02, 03, 04)
    """
    now = datetime.datetime.now()
    year = now.year - 1911  # è½‰æ›ç‚ºæ°‘åœ‹å¹´
    month = now.month
    
    if month <= 3:
        season = "04"
        year -= 1  # å‰ä¸€å¹´ç¬¬å››å­£
    elif month <= 6:
        season = "01"  # ç•¶å¹´ç¬¬ä¸€å­£
    elif month <= 9:
        season = "02"  # ç•¶å¹´ç¬¬äºŒå­£
    else:
        season = "03"  # ç•¶å¹´ç¬¬ä¸‰å­£
        
    return str(year), season

# å…¨å±€æ•¸æ“šç²å–ç‹€æ…‹è·Ÿè¸ª
data_fetch_status = {
    "last_fetch_time": None,
    "last_fetch_source": None,
    "successful_sources": [],
    "failed_sources": []
}

def get_eps_data(use_cache=True, cache_expiry_hours=72):
    """
    æŠ“å–æ‰€æœ‰ä¸Šå¸‚å…¬å¸çš„ EPS å’Œè‚¡æ¯è³‡æ–™ï¼Œå¢åŠ å¤šä¾†æºç²å–å’Œå¼·åŒ–éŒ¯èª¤è™•ç†
    
    åƒæ•¸:
    - use_cache: æ˜¯å¦ä½¿ç”¨ç·©å­˜
    - cache_expiry_hours: ç·©å­˜æœ‰æ•ˆæ™‚é–“ï¼ˆå°æ™‚ï¼‰
    
    è¿”å›:
    - å­—å…¸: {stock_id: {"eps": value, "dividend": value}}
    """
    global data_fetch_status
    
    # æª¢æŸ¥ç·©å­˜ï¼ˆå¢åŠ ç·©å­˜æœ‰æ•ˆæœŸè‡³72å°æ™‚ï¼‰
    cache_file = os.path.join(CACHE_DIR, 'eps_data_cache.json')
    if use_cache and os.path.exists(cache_file):
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                cache_time = datetime.datetime.fromisoformat(cache_data['timestamp'])
                
                # æª¢æŸ¥ç·©å­˜æ˜¯å¦éæœŸï¼ˆå»¶é•·è‡³72å°æ™‚ï¼‰
                if datetime.datetime.now() - cache_time < datetime.timedelta(hours=cache_expiry_hours):
                    print(f"[scraper] âœ… ä½¿ç”¨ç·©å­˜çš„ EPS å’Œè‚¡æ¯æ•¸æ“š (æ›´æ–°æ–¼ {cache_time.strftime('%Y-%m-%d %H:%M')})")
                    return cache_data['data']
        except Exception as e:
            print(f"[scraper] âš ï¸ è®€å–ç·©å­˜å¤±æ•—: {e}")
    
    # ä¸¦è¡Œå¾å¤šå€‹ä¾†æºç²å–æ•¸æ“š
    print("[scraper] ğŸ”„ å¾å¤šå€‹æ•¸æ“šæºä¸¦è¡Œç²å– EPS å’Œè‚¡æ¯æ•¸æ“š...")
    
    # å®šç¾©æ•¸æ“šæºå‡½æ•¸åˆ—è¡¨
    data_sources = [
        ("MOPS", get_eps_data_from_mops),
        ("Yahoo Finance", get_eps_data_from_yahoo),
        ("Backup", get_backup_eps_data)
    ]
    
    # è¿½è¸ªçµæœ
    results = {}
    successful_source = None
    
    # ä½¿ç”¨ç·šç¨‹æ± ä¸¦è¡ŒåŸ·è¡Œæ•¸æ“šç²å–
    with ThreadPoolExecutor(max_workers=len(data_sources)) as executor:
        # æäº¤æ‰€æœ‰ä»»å‹™
        future_to_source = {executor.submit(source_func): source_name for source_name, source_func in data_sources}
        
        # ç•¶ä»»ä½•ä¸€å€‹ä»»å‹™å®Œæˆæ™‚è™•ç†çµæœ
        for future in as_completed(future_to_source):
            source_name = future_to_source[future]
            try:
                data = future.result()
                if data and len(data) > 0:
                    print(f"[scraper] âœ… å¾ {source_name} æˆåŠŸç²å– {len(data)} æª”è‚¡ç¥¨çš„ EPS å’Œè‚¡æ¯æ•¸æ“š")
                    results = data
                    successful_source = source_name
                    data_fetch_status["successful_sources"].append(source_name)
                    
                    # ä¸­æ–·å…¶ä»–ä»åœ¨åŸ·è¡Œçš„ä»»å‹™
                    for f in future_to_source:
                        if not f.done():
                            f.cancel()
                    break
                else:
                    print(f"[scraper] âš ï¸ å¾ {source_name} ç²å–æ•¸æ“šå¤±æ•—æˆ–ç‚ºç©º")
                    data_fetch_status["failed_sources"].append(source_name)
            except Exception as e:
                print(f"[scraper] âŒ å¾ {source_name} ç²å–æ•¸æ“šæ™‚å‡ºéŒ¯: {e}")
                data_fetch_status["failed_sources"].append(source_name)
    
    # å¦‚æœæ‰€æœ‰ä¾†æºéƒ½å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ
    if not results:
        print("[scraper] âš ï¸ æ‰€æœ‰æ•¸æ“šæºå‡å¤±æ•—ï¼Œä½¿ç”¨ç¡¬ç·¨ç¢¼çš„å‚™ç”¨æ•¸æ“š")
        results = get_hardcoded_eps_data()
        successful_source = "Hardcoded Backup"
    
    # æ›´æ–°ç‹€æ…‹è¿½è¸ª
    data_fetch_status["last_fetch_time"] = datetime.datetime.now().isoformat()
    data_fetch_status["last_fetch_source"] = successful_source
    
    # è¨˜éŒ„æ•¸æ“šç²å–ç‹€æ…‹
    try:
        status_file = os.path.join(CACHE_DIR, 'data_fetch_status.json')
        with open(status_file, 'w', encoding='utf-8') as f:
            json.dump(data_fetch_status, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"[scraper] âš ï¸ ç„¡æ³•ä¿å­˜æ•¸æ“šç²å–ç‹€æ…‹: {e}")
    
    # å„²å­˜çµæœåˆ°ç·©å­˜
    if use_cache and results:
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                cache_data = {
                    'timestamp': datetime.datetime.now().isoformat(),
                    'source': successful_source,
                    'data': results
                }
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            print(f"[scraper] âœ… å·²æ›´æ–° EPS å’Œè‚¡æ¯æ•¸æ“šç·©å­˜")
        except Exception as e:
            print(f"[scraper] âš ï¸ å¯«å…¥ç·©å­˜å¤±æ•—: {e}")
    
    return results

def get_eps_data_from_mops():
    """å¾å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™ç²å– EPS æ•¸æ“šï¼ˆè¨­ç½®å„ªåŒ–çš„è¶…æ™‚å’Œé‡è©¦æ©Ÿåˆ¶ï¼‰"""
    year, season = get_latest_season()
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36",
        "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
        "Referer": "https://mops.twse.com.tw/mops/web/t05st09_1"
    }

    print(f"[scraper] å˜—è©¦ç²å– {year} å¹´ç¬¬ {season} å­£çš„ EPS æ•¸æ“š...")
    
    # åˆå§‹åŒ–çµæœå­—å…¸å’Œæ•¸æ“šæ¡†
    result = {}
    eps_df = pd.DataFrame()
    div_df = pd.DataFrame()
    
    # ä½¿ç”¨å„ªåŒ–çš„é‡è©¦æ©Ÿåˆ¶å’Œæ›´çŸ­çš„è¶…æ™‚è¨­å®š - æ¸›å°‘æ•´é«”ç­‰å¾…æ™‚é–“
    session = create_retry_session(retries=2, backoff_factor=0.3)
    
    # æª¢æŸ¥é€£æ¥æ˜¯å¦å¯ç”¨
    try:
        socket_timeout = 5  # ä½¿ç”¨éå¸¸çŸ­çš„è¶…æ™‚æ¸¬è©¦é€£æ¥
        host = "mops.twse.com.tw"
        port = 443
        
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(socket_timeout)
        result = sock.connect_ex((host, port))
        sock.close()
        
        if result != 0:
            print(f"[scraper] âš ï¸ ç„¡æ³•é€£æ¥åˆ° {host}:{port}ï¼Œç‹€æ…‹ç¢¼: {result}")
            return {}
    except Exception as e:
        print(f"[scraper] âš ï¸ é€£æ¥æ¸¬è©¦å¤±æ•—: {e}")
        return {}
    
    # æˆåŠŸé€£æ¥å¾Œï¼Œå˜—è©¦ç²å–æ•¸æ“š
    try:
        # å˜—è©¦ç²å–æ¯è‚¡ç›ˆé¤˜è³‡æ–™ï¼Œè¨­ç½®æ›´çŸ­çš„è¶…æ™‚
        eps_url = "https://mops.twse.com.tw/mops/web/ajax_t05st09_1"
        eps_data = {
            "encodeURIComponent": "1",
            "step": "1",
            "firstin": "1",
            "off": "1",
            "TYPEK": "sii",
            "year": year,
            "season": season
        }
        
        # å¢åŠ é‡è©¦é‚è¼¯ä½†ç¸®çŸ­è¶…æ™‚æ™‚é–“å’Œé‡è©¦æ¬¡æ•¸
        max_attempts = 2
        for attempt in range(max_attempts):
            try:
                # ç™¼é€è«‹æ±‚ï¼Œä½¿ç”¨è¼ƒçŸ­çš„è¶…æ™‚
                eps_res = session.post(
                    eps_url,
                    data=eps_data,
                    headers=headers,
                    timeout=(5, 10)  # é€£æ¥è¶…æ™‚5ç§’ï¼Œè®€å–è¶…æ™‚10ç§’
                )
                
                if eps_res.status_code == 200 and "<table" in eps_res.text.lower():
                    try:
                        tables = pd.read_html(StringIO(eps_res.text))
                        if len(tables) > 1:
                            eps_df = tables[1]
                            eps_df.columns = eps_df.columns.str.strip()
                            eps_df = eps_df.rename(columns={"å…¬å¸ä»£è™Ÿ": "stock_id", "åŸºæœ¬æ¯è‚¡ç›ˆé¤˜ï¼ˆå…ƒï¼‰": "EPS"})
                            eps_df = eps_df[["stock_id", "EPS"]].dropna()
                            eps_df["EPS"] = pd.to_numeric(eps_df["EPS"], errors="coerce")
                            eps_df = eps_df.dropna()
                            break  # æˆåŠŸç²å–æ•¸æ“šï¼Œè·³å‡ºé‡è©¦å¾ªç’°
                    except Exception as e:
                        print(f"[scraper] âš ï¸ EPSè¡¨æ ¼è§£æå¤±æ•— (å˜—è©¦ {attempt+1}/{max_attempts}): {e}")
                
                # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€æ¬¡å˜—è©¦ï¼Œæš«åœä¸€ä¸‹å†é‡è©¦ï¼ˆç¸®çŸ­ç­‰å¾…æ™‚é–“ï¼‰
                if attempt < max_attempts - 1:
                    time.sleep(1)  # æ¸›å°‘å»¶é²
            except Exception as e:
                print(f"[scraper] âš ï¸ EPSæ•¸æ“šè«‹æ±‚å¤±æ•— (å˜—è©¦ {attempt+1}/{max_attempts}): {e}")
                if attempt < max_attempts - 1:
                    time.sleep(1)
    except Exception as e:
        print(f"[scraper] âŒ æŸ¥ç„¡ EPS è¡¨æ ¼æˆ–æ ¼å¼éŒ¯èª¤ï¼š{e}")
    
    # åŒæ¨£çš„æ–¹å¼è™•ç†è‚¡æ¯è³‡æ–™ï¼Œä½†ç¸®çŸ­ç­‰å¾…æ™‚é–“
    try:
        max_attempts = 2
        for attempt in range(max_attempts):
            try:
                # å˜—è©¦ç²å–è‚¡æ¯è³‡æ–™ï¼Œæ¸›å°‘è¶…æ™‚æ™‚é–“
                div_res = session.post(
                    "https://mops.twse.com.tw/mops/web/ajax_t05st34",
                    data={"encodeURIComponent": "1", "step": "1", "firstin": "1", "off": "1", "TYPEK": "sii"},
                    headers=headers, 
                    timeout=(5, 10)  # æ¸›å°‘è¶…æ™‚æ™‚é–“
                )
                
                if div_res.status_code == 200 and "<table" in div_res.text.lower():
                    try:
                        tables = pd.read_html(StringIO(div_res.text))
                        if len(tables) > 1:
                            div_df = tables[1]
                            div_df.columns = div_df.columns.str.strip()
                            div_df = div_df.rename(columns={"å…¬å¸ä»£è™Ÿ": "stock_id", "ç¾é‡‘è‚¡åˆ©": "Dividend"})
                            div_df = div_df[["stock_id", "Dividend"]].dropna()
                            div_df["Dividend"] = pd.to_numeric(div_df["Dividend"], errors="coerce")
                            div_df = div_df.dropna()
                            break  # æˆåŠŸç²å–æ•¸æ“šï¼Œè·³å‡ºé‡è©¦å¾ªç’°
                    except Exception as e:
                        print(f"[scraper] âš ï¸ è‚¡æ¯è¡¨æ ¼è§£æå¤±æ•— (å˜—è©¦ {attempt+1}/{max_attempts}): {e}")
                
                # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€æ¬¡å˜—è©¦ï¼Œæš«åœä¸€ä¸‹å†é‡è©¦
                if attempt < max_attempts - 1:
                    time.sleep(1)  # æ¸›å°‘å»¶é²
            except Exception as e:
                print(f"[scraper] âš ï¸ è‚¡æ¯æ•¸æ“šè«‹æ±‚å¤±æ•— (å˜—è©¦ {attempt+1}/{max_attempts}): {e}")
                if attempt < max_attempts - 1:
                    time.sleep(1)
    except Exception as e:
        print(f"[scraper] âŒ æŸ¥ç„¡è‚¡åˆ©è¡¨æ ¼æˆ–æ ¼å¼éŒ¯èª¤ï¼š{e}")
    
    # æª¢æŸ¥æ˜¯å¦æˆåŠŸç²å–æ•¸æ“š
    if eps_df.empty and div_df.empty:
        print("[scraper] âš ï¸ ç„¡æ³•å¾å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™ç²å–æ•¸æ“š")
        return {}
    
    # åˆä½µæ•¸æ“š
    for _, row in eps_df.iterrows():
        try:
            sid = str(row["stock_id"]).zfill(4)
            result[sid] = {"eps": round(row["EPS"], 2), "dividend": None}
        except:
            continue

    for _, row in div_df.iterrows():
        try:
            sid = str(row["stock_id"]).zfill(4)
            if sid not in result:
                result[sid] = {"eps": None, "dividend": None}
            result[sid]["dividend"] = round(row["Dividend"], 2)
        except:
            continue
    
    print(f"[scraper] âœ… æˆåŠŸå¾å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™ç²å– {len(result)} æª”è‚¡ç¥¨çš„ EPS å’Œè‚¡æ¯æ•¸æ“š")
    return result

def get_eps_data_from_yahoo():
    """å¾ Yahoo Finance ç²å– EPS å’Œè‚¡æ¯æ•¸æ“š"""
    try:
        # å°å…¥ finance_yahoo æ¨¡çµ„ä¸­çš„å‡½æ•¸
        from modules.data.finance_yahoo import get_eps_data_alternative
        
        # ä½¿ç”¨ç¸®çŸ­è¶…æ™‚çš„è¨­ç½®ä¾†èª¿ç”¨æ­¤å‡½æ•¸ï¼Œä¸¦æŒ‡å®šæ›´å°çš„è™•ç†æ‰¹æ¬¡ä¾†é¿å…é€Ÿç‡é™åˆ¶
        return get_eps_data_alternative(max_stocks=40, timeout=15, batch_size=3, batch_delay=2.0)
    except Exception as e:
        print(f"[scraper] âŒ ä½¿ç”¨ Yahoo Finance ç²å–æ•¸æ“šå¤±æ•—ï¼š{e}")
        return {}

def get_backup_eps_data():
    """å˜—è©¦å¾å‚™ç”¨ä¾†æºç²å– EPS æ•¸æ“š"""
    try:
        # å…ˆæª¢æŸ¥å‚™ç”¨ç·©å­˜ï¼Œå³ä½¿éæœŸä¹Ÿä½¿ç”¨
        backup_cache_file = os.path.join(CACHE_DIR, 'backup_eps_data_cache.json')
        if os.path.exists(backup_cache_file):
            try:
                with open(backup_cache_file, 'r', encoding='utf-8') as f:
                    backup_data = json.load(f)
                    # é¡¯ç¤ºç·©å­˜å¹´é½¡
                    cache_time = datetime.datetime.fromisoformat(backup_data['timestamp'])
                    age_hours = (datetime.datetime.now() - cache_time).total_seconds() / 3600
                    print(f"[scraper] â„¹ï¸ ä½¿ç”¨å‚™ç”¨ç·©å­˜çš„ EPS æ•¸æ“š (å¹´é½¡ï¼š{age_hours:.1f}å°æ™‚)")
                    return backup_data['data']
            except Exception as e:
                print(f"[scraper] âš ï¸ è®€å–å‚™ç”¨ç·©å­˜å¤±æ•—: {e}")
        
        # å¦å‰‡ä½¿ç”¨ç¡¬ç·¨ç¢¼çš„å‚™ç”¨æ•¸æ“š
        return get_hardcoded_eps_data()
    except Exception as e:
        print(f"[scraper] âŒ å‚™ä»½æ•¸æ“šæºå¤±æ•—: {e}")
        return {}

def get_hardcoded_eps_data():
    """æä¾›ç¡¬ç·¨ç¢¼çš„é‡è¦è‚¡ç¥¨ EPS å’Œè‚¡æ¯æ•¸æ“šä½œç‚ºæœ€å¾Œçš„å‚™ç”¨æ–¹æ¡ˆ"""
    print("[scraper] ä½¿ç”¨ç¡¬ç·¨ç¢¼çš„é‡è¦è‚¡ç¥¨ EPS å’Œè‚¡æ¯æ•¸æ“š")
    
    # å¤§å‹è‚¡çš„åŸºæœ¬è²¡å‹™æ•¸æ“š - 2025å¹´æ›´æ–°çš„æ•¸æ“š
    return {
        "2330": {"eps": 9.8, "dividend": 3.2},   # å°ç©é›»
        "2317": {"eps": 5.5, "dividend": 4.7},   # é´»æµ·
        "2454": {"eps": 52.0, "dividend": 53.0}, # è¯ç™¼ç§‘
        "2412": {"eps": 4.7, "dividend": 5.0},   # ä¸­è¯é›»
        "2303": {"eps": 2.3, "dividend": 2.1},   # è¯é›»
        "2308": {"eps": 6.0, "dividend": 4.4},   # å°é”é›»
        "2882": {"eps": 2.2, "dividend": 2.9},   # åœ‹æ³°é‡‘
        "2881": {"eps": 2.2, "dividend": 2.6},   # å¯Œé‚¦é‡‘
        "1301": {"eps": 5.0, "dividend": 4.0},   # å°å¡‘
        "1303": {"eps": 4.2, "dividend": 3.6},   # å—äº
        "2002": {"eps": 1.9, "dividend": 2.3},   # ä¸­é‹¼
        "2886": {"eps": 2.0, "dividend": 2.5},   # å…†è±é‡‘
        "1216": {"eps": 3.8, "dividend": 3.6},   # çµ±ä¸€
        "2891": {"eps": 1.9, "dividend": 2.3},   # ä¸­ä¿¡é‡‘
        "3008": {"eps": 4.7, "dividend": 4.3},   # å¤§ç«‹å…‰
        "2884": {"eps": 1.8, "dividend": 1.9},   # ç‰å±±é‡‘
        "2327": {"eps": 15.0, "dividend": 9.0},  # åœ‹å·¨
        "2603": {"eps": 2.5, "dividend": 2.7},   # é•·æ¦®
        "3045": {"eps": 5.4, "dividend": 4.7},   # å°ç£å¤§
        "2912": {"eps": 7.8, "dividend": 7.0},   # çµ±ä¸€è¶…
        "2382": {"eps": 5.2, "dividend": 4.0},   # å»£é”
        "2609": {"eps": 3.3, "dividend": 2.8},   # é™½æ˜
        "6505": {"eps": 6.9, "dividend": 5.5},   # å°å¡‘åŒ–
        "2892": {"eps": 1.8, "dividend": 1.7},   # ç¬¬ä¸€é‡‘
        "2887": {"eps": 1.6, "dividend": 1.5},   # å°æ–°é‡‘
        "1101": {"eps": 1.8, "dividend": 1.6},   # å°æ³¥
        "3711": {"eps": 13.8, "dividend": 7.0},  # æ—¥æœˆå…‰æŠ•æ§
        "2615": {"eps": 2.7, "dividend": 2.2},   # è¬æµ·
        "2345": {"eps": 5.6, "dividend": 4.5},   # æ™ºé‚¦
        "5880": {"eps": 3.2, "dividend": 2.9}    # åˆåº«é‡‘
    }


def get_all_valid_twse_stocks(limit=None, use_cache=True, cache_expiry_hours=48):
    """
    å¾è­‰äº¤æ‰€ç²å–æ‰€æœ‰æœ‰æ•ˆçš„ä¸Šå¸‚è‚¡ç¥¨ï¼Œå¢åŠ ç·©å­˜æ©Ÿåˆ¶
    
    åƒæ•¸:
    - limit: é™åˆ¶è¿”å›çš„è‚¡ç¥¨æ•¸é‡ï¼ŒNone è¡¨ç¤ºä¸é™åˆ¶
    - use_cache: æ˜¯å¦ä½¿ç”¨ç·©å­˜
    - cache_expiry_hours: ç·©å­˜æœ‰æ•ˆæ™‚é–“ï¼ˆå°æ™‚ï¼‰
    
    è¿”å›:
    - è‚¡ç¥¨è³‡è¨Šåˆ—è¡¨ [{"stock_id": id, "stock_name": name, "market_type": type, "industry": ind}]
    """
    # æª¢æŸ¥ç·©å­˜
    cache_file = os.path.join(CACHE_DIR, 'twse_stocks_cache.json')
    if use_cache and os.path.exists(cache_file):
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                cache_time = datetime.datetime.fromisoformat(cache_data['timestamp'])
                
                # æª¢æŸ¥ç·©å­˜æ˜¯å¦éæœŸ
                if datetime.datetime.now() - cache_time < datetime.timedelta(hours=cache_expiry_hours):
                    print(f"[scraper] âœ… ä½¿ç”¨ç·©å­˜çš„è‚¡ç¥¨åˆ—è¡¨ (æ›´æ–°æ–¼ {cache_time.strftime('%Y-%m-%d %H:%M')})")
                    
                    # åœ¨è¿”å›ç·©å­˜çµæœå‰å¢åŠ é™åˆ¶æª¢æŸ¥
                    if limit is not None and isinstance(limit, int) and limit > 0:
                        print(f"[scraper] é™åˆ¶è¿”å› {limit} æª”è‚¡ç¥¨")
                        return cache_data['data'][:limit]
                    
                    return cache_data['data']
        except Exception as e:
            print(f"[scraper] âš ï¸ è®€å–è‚¡ç¥¨åˆ—è¡¨ç·©å­˜å¤±æ•—: {e}")
    
    url = "https://isin.twse.com.tw/isin/C_public.jsp?strMode=2"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36",
        "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7"
    }
    
    # ä½¿ç”¨å„ªåŒ–çš„é‡è©¦è¨­ç½®
    session = create_retry_session(retries=2, backoff_factor=0.5)
    
    try:
        # å¢åŠ é‡è©¦é‚è¼¯ï¼Œä½†ä½¿ç”¨æ›´çŸ­çš„è¶…æ™‚
        max_attempts = 2
        for attempt in range(max_attempts):
            try:
                response = session.get(url, headers=headers, timeout=(5, 15))
                response.encoding = 'big5'
                
                if response.status_code == 200 and len(response.text) > 1000:
                    break  # æˆåŠŸç²å–æ•¸æ“š
                    
                # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€æ¬¡å˜—è©¦ï¼Œæš«åœä¸€ä¸‹å†é‡è©¦
                if attempt < max_attempts - 1:
                    time.sleep(1.5)
            except Exception as e:
                print(f"[scraper] âš ï¸ ç²å–è‚¡ç¥¨åˆ—è¡¨å¤±æ•— (å˜—è©¦ {attempt+1}/{max_attempts}): {e}")
                if attempt < max_attempts - 1:
                    time.sleep(1.5)
                else:
                    # å¦‚æœæœ€å¾Œä¸€æ¬¡å˜—è©¦ä»ç„¶å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨åˆ—è¡¨
                    print(f"[scraper] âš ï¸ ç²å–è‚¡ç¥¨åˆ—è¡¨å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨åˆ—è¡¨")
                    backup_stocks = get_backup_stock_list()
                    
                    # åœ¨è¿”å›å‚™ç”¨åˆ—è¡¨å‰å¢åŠ é™åˆ¶æª¢æŸ¥
                    if limit is not None and isinstance(limit, int) and limit > 0:
                        print(f"[scraper] é™åˆ¶è¿”å› {limit} æª”å‚™ç”¨è‚¡ç¥¨")
                        return backup_stocks[:limit]
                    
                    return backup_stocks

        # è§£ææ•¸æ“š
        try:
            tables = pd.read_html(StringIO(response.text))
            df = tables[0]
            df.columns = df.iloc[0]
            df = df[1:]

            all_stocks = []
            for _, row in df.iterrows():
                if pd.isna(row["æœ‰åƒ¹è­‰åˆ¸ä»£è™ŸåŠåç¨±"]):
                    continue
                    
                parts = str(row["æœ‰åƒ¹è­‰åˆ¸ä»£è™ŸåŠåç¨±"]).split()
                if len(parts) != 2:
                    continue
                    
                stock_id, stock_name = parts
                market_type = str(row["å¸‚å ´åˆ¥"])
                industry = str(row["ç”¢æ¥­åˆ¥"])

                # ç¯©é¸ä¸Šå¸‚è‚¡ç¥¨ï¼Œæ’é™¤ä¸‹å¸‚ã€ç©ºç™½ä»£ç¢¼
                if not stock_id.isdigit():
                    continue

                # æ’é™¤å·²ä¸‹å¸‚æˆ–ç‰¹åˆ¥æ¨™è¨˜çš„è‚¡ç¥¨
                if "ä¸‹å¸‚" in stock_name:
                    continue

                all_stocks.append({
                    "stock_id": stock_id,
                    "stock_name": stock_name,
                    "market_type": market_type,
                    "industry": industry
                })

            print(f"[scraper] âœ… æˆåŠŸç²å– {len(all_stocks)} æª”ä¸Šå¸‚è‚¡ç¥¨åˆ—è¡¨")
            
            # å„²å­˜çµæœåˆ°ç·©å­˜
            if use_cache and all_stocks:
                try:
                    with open(cache_file, 'w', encoding='utf-8') as f:
                        cache_data = {
                            'timestamp': datetime.datetime.now().isoformat(),
                            'data': all_stocks
                        }
                        json.dump(cache_data, f, ensure_ascii=False, indent=2)
                    print(f"[scraper] âœ… å·²æ›´æ–°è‚¡ç¥¨åˆ—è¡¨ç·©å­˜")
                except Exception as e:
                    print(f"[scraper] âš ï¸ å¯«å…¥è‚¡ç¥¨åˆ—è¡¨ç·©å­˜å¤±æ•—: {e}")
            
            # åœ¨è¿”å›çµæœå‰å¢åŠ é™åˆ¶æª¢æŸ¥
            if limit is not None and isinstance(limit, int) and limit > 0:
                print(f"[scraper] é™åˆ¶è¿”å› {limit} æª”è‚¡ç¥¨")
                return all_stocks[:limit]
            
            return all_stocks
        except Exception as e:
            print(f"[scraper] âš ï¸ è§£æè‚¡ç¥¨åˆ—è¡¨æ•¸æ“šå¤±æ•—: {e}")
            # å¦‚æœè§£æå¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨åˆ—è¡¨
            backup_stocks = get_backup_stock_list()
            
            # åœ¨è¿”å›å‚™ç”¨åˆ—è¡¨å‰å¢åŠ é™åˆ¶æª¢æŸ¥
            if limit is not None and isinstance(limit, int) and limit > 0:
                print(f"[scraper] é™åˆ¶è¿”å› {limit} æª”å‚™ç”¨è‚¡ç¥¨")
                return backup_stocks[:limit]
            
            return backup_stocks
            
    except Exception as e:
        print(f"[scraper] âŒ ç²å–ä¸Šå¸‚è‚¡ç¥¨åˆ—è¡¨å¤±æ•—ï¼š{e}")
        # å¦‚æœå¤±æ•—ï¼Œè¿”å›ä¸€å€‹åŒ…å«ä¸»è¦ä¸Šå¸‚å…¬å¸çš„å‚™ç”¨åˆ—è¡¨
        print(f"[scraper] âš ï¸ ä½¿ç”¨å‚™ç”¨ä¸Šå¸‚è‚¡ç¥¨åˆ—è¡¨...")
        backup_stocks = get_backup_stock_list()
        
        # åœ¨è¿”å›å‚™ç”¨åˆ—è¡¨å‰å¢åŠ é™åˆ¶æª¢æŸ¥
        if limit is not None and isinstance(limit, int) and limit > 0:
            print(f"[scraper] é™åˆ¶è¿”å› {limit} æª”å‚™ç”¨è‚¡ç¥¨")
            return backup_stocks[:limit]
        
        return backup_stocks


def get_backup_stock_list():
    """æä¾›å‚™ç”¨çš„ä¸Šå¸‚è‚¡ç¥¨åˆ—è¡¨ - æ›´æ–°è‡³2025å¹´ç‰ˆæœ¬"""
    backup_stocks = [
        {"stock_id": "2330", "stock_name": "å°ç©é›»", "market_type": "ä¸Šå¸‚", "industry": "åŠå°é«”æ¥­"},
        {"stock_id": "2317", "stock_name": "é´»æµ·", "market_type": "ä¸Šå¸‚", "industry": "é›»å­é›¶çµ„ä»¶æ¥­"},
        {"stock_id": "2303", "stock_name": "è¯é›»", "market_type": "ä¸Šå¸‚", "industry": "åŠå°é«”æ¥­"},
        {"stock_id": "2308", "stock_name": "å°é”é›»", "market_type": "ä¸Šå¸‚", "industry": "é›»å­é›¶çµ„ä»¶æ¥­"},
        {"stock_id": "2454", "stock_name": "è¯ç™¼ç§‘", "market_type": "ä¸Šå¸‚", "industry": "åŠå°é«”æ¥­"},
        {"stock_id": "2412", "stock_name": "ä¸­è¯é›»", "market_type": "ä¸Šå¸‚", "industry": "é›»ä¿¡æ¥­"},
        {"stock_id": "2882", "stock_name": "åœ‹æ³°é‡‘", "market_type": "ä¸Šå¸‚", "industry": "é‡‘èæ¥­"},
        {"stock_id": "1301", "stock_name": "å°å¡‘", "market_type": "ä¸Šå¸‚", "industry": "å¡‘è† å·¥æ¥­"},
        {"stock_id": "1303", "stock_name": "å—äº", "market_type": "ä¸Šå¸‚", "industry": "å¡‘è† å·¥æ¥­"},
        {"stock_id": "2881", "stock_name": "å¯Œé‚¦é‡‘", "market_type": "ä¸Šå¸‚", "industry": "é‡‘èæ¥­"},
        {"stock_id": "1216", "stock_name": "çµ±ä¸€", "market_type": "ä¸Šå¸‚", "industry": "é£Ÿå“å·¥æ¥­"},
        {"stock_id": "2002", "stock_name": "ä¸­é‹¼", "market_type": "ä¸Šå¸‚", "industry": "é‹¼éµå·¥æ¥­"},
        {"stock_id": "2886", "stock_name": "å…†è±é‡‘", "market_type": "ä¸Šå¸‚", "industry": "é‡‘èæ¥­"},
        {"stock_id": "1101", "stock_name": "å°æ³¥", "market_type": "ä¸Šå¸‚", "industry": "æ°´æ³¥å·¥æ¥­"},
        {"stock_id": "2891", "stock_name": "ä¸­ä¿¡é‡‘", "market_type": "ä¸Šå¸‚", "industry": "é‡‘èæ¥­"},
        {"stock_id": "3711", "stock_name": "æ—¥æœˆå…‰æŠ•æ§", "market_type": "ä¸Šå¸‚", "industry": "åŠå°é«”æ¥­"},
        {"stock_id": "2327", "stock_name": "åœ‹å·¨", "market_type": "ä¸Šå¸‚", "industry": "é›»å­é›¶çµ„ä»¶æ¥­"},
        {"stock_id": "2912", "stock_name": "çµ±ä¸€è¶…", "market_type": "ä¸Šå¸‚", "industry": "è²¿æ˜“ç™¾è²¨"},
        {"stock_id": "2207", "stock_name": "å’Œæ³°è»Š", "market_type": "ä¸Šå¸‚", "industry": "æ±½è»Šå·¥æ¥­"},
        {"stock_id": "2884", "stock_name": "ç‰å±±é‡‘", "market_type": "ä¸Šå¸‚", "industry": "é‡‘èæ¥­"},
        {"stock_id": "2382", "stock_name": "å»£é”", "market_type": "ä¸Šå¸‚", "industry": "é›»è…¦åŠé€±é‚Šè¨­å‚™æ¥­"},
        {"stock_id": "2609", "stock_name": "é™½æ˜", "market_type": "ä¸Šå¸‚", "industry": "èˆªé‹æ¥­"},
        {"stock_id": "6505", "stock_name": "å°å¡‘åŒ–", "market_type": "ä¸Šå¸‚", "industry": "çŸ³æ²¹ã€ç…¤è£½å“æ¥­"},
        {"stock_id": "2892", "stock_name": "ç¬¬ä¸€é‡‘", "market_type": "ä¸Šå¸‚", "industry": "é‡‘èæ¥­"},
        {"stock_id": "2887", "stock_name": "å°æ–°é‡‘", "market_type": "ä¸Šå¸‚", "industry": "é‡‘èæ¥­"},
        {"stock_id": "2345", "stock_name": "æ™ºé‚¦", "market_type": "ä¸Šå¸‚", "industry": "é€šä¿¡ç¶²è·¯æ¥­"},
        {"stock_id": "3008", "stock_name": "å¤§ç«‹å…‰", "market_type": "ä¸Šå¸‚", "industry": "å…‰é›»æ¥­"},
        {"stock_id": "2615", "stock_name": "è¬æµ·", "market_type": "ä¸Šå¸‚", "industry": "èˆªé‹æ¥­"},
        {"stock_id": "5880", "stock_name": "åˆåº«é‡‘", "market_type": "ä¸Šå¸‚", "industry": "é‡‘èæ¥­"},
        {"stock_id": "3045", "stock_name": "å°ç£å¤§", "market_type": "ä¸Šå¸‚", "industry": "é›»ä¿¡æ¥­"}
    ]
    return backup_stocks


def get_dividend_data(use_cache=True, cache_expiry_hours=72):
    """
    åƒ…ç²å–è‚¡æ¯è³‡æ–™ï¼Œå¢åŠ ç·©å­˜æœ‰æ•ˆæœŸ
    
    åƒæ•¸:
    - use_cache: æ˜¯å¦ä½¿ç”¨ç·©å­˜
    - cache_expiry_hours: ç·©å­˜æœ‰æ•ˆæ™‚é–“ï¼ˆå°æ™‚ï¼‰
    
    è¿”å›:
    - å­—å…¸: {stock_id: dividend_value}
    """
    # æª¢æŸ¥ç·©å­˜
    cache_file = os.path.join(CACHE_DIR, 'dividend_data_cache.json')
    if use_cache and os.path.exists(cache_file):
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                cache_time = datetime.datetime.fromisoformat(cache_data['timestamp'])
                
                # æª¢æŸ¥ç·©å­˜æ˜¯å¦éæœŸï¼Œå»¶é•·åˆ°72å°æ™‚
                if datetime.datetime.now() - cache_time < datetime.timedelta(hours=cache_expiry_hours):
                    print(f"[scraper] âœ… ä½¿ç”¨ç·©å­˜çš„è‚¡æ¯æ•¸æ“š (æ›´æ–°æ–¼ {cache_time.strftime('%Y-%m-%d %H:%M')})")
                    return cache_data['data']
        except Exception as e:
            print(f"[scraper] âš ï¸ è®€å–è‚¡æ¯ç·©å­˜å¤±æ•—: {e}")
    
    # ç²å–å®Œæ•´ EPS å’Œè‚¡æ¯æ•¸æ“š
    all_data = get_eps_data(use_cache, cache_expiry_hours)
    
    # æå–è‚¡æ¯æ•¸æ“š
    dividend_data = {sid: val["dividend"] for sid, val in all_data.items() if val["dividend"] is not None}
    
    # å„²å­˜çµæœåˆ°ç·©å­˜
    if use_cache and dividend_data:
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                cache_data = {
                    'timestamp': datetime.datetime.now().isoformat(),
                    'data': dividend_data
                }
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            print(f"[scraper] âœ… å·²æ›´æ–°è‚¡æ¯æ•¸æ“šç·©å­˜")
        except Exception as e:
            print(f"[scraper] âš ï¸ å¯«å…¥è‚¡æ¯ç·©å­˜å¤±æ•—: {e}")
    
    return dividend_data


def get_all_valid_twse_stocks_with_type(use_cache=True):
    """
    ç²å–æ‰€æœ‰ä¸Šå¸‚è‚¡ç¥¨ï¼Œä¸¦æ·»åŠ è‚¡ç¥¨é¡å‹æ¨™è¨˜ï¼ˆå¤§å‹è‚¡ã€ä¸­å°å‹è‚¡ã€ETFï¼‰
    
    åƒæ•¸:
    - use_cache: æ˜¯å¦ä½¿ç”¨ç·©å­˜
    
    è¿”å›:
    - æ·»åŠ äº†é¡å‹çš„è‚¡ç¥¨åˆ—è¡¨
    """
    from modules.data.fetcher import is_etf
    
    # ç²å–åŸå§‹è‚¡ç¥¨åˆ—è¡¨
    raw = get_all_valid_twse_stocks(use_cache)
    stocks = []
    
    for item in raw:
        stock_id = item["stock_id"]
        stock_name = item["stock_name"]
        
        # åˆ¤æ–·è‚¡ç¥¨é¡å‹
        if is_etf(stock_name):
            stock_type = "etf"
        elif int(stock_id) < 4000:
            stock_type = "large"  # ä¸€èˆ¬èªç‚ºç·¨è™Ÿå°æ–¼4000çš„å¤šç‚ºå¤§å‹è‚¡
        else:
            stock_type = "small"  # ç·¨è™Ÿå¤§æ–¼4000å¤šç‚ºä¸­å°å‹è‚¡
            
        stocks.append({
            "stock_id": stock_id, 
            "stock_name": stock_name, 
            "type": stock_type,
            "industry": item["industry"]
        })
        
    return stocks

def fetch_fundamental_data(stock_ids, max_stocks=20):
    """
    ç²å–åŸºæœ¬é¢æ•¸æ“šï¼ˆPE, PB, ROE, æ³•äººæŒè‚¡ç­‰ï¼‰ï¼Œå¢åŠ å¹³è¡Œè™•ç†å’Œè¶…æ™‚æ§åˆ¶
    
    åƒæ•¸:
    - stock_ids: è‚¡ç¥¨ä»£ç¢¼åˆ—è¡¨
    - max_stocks: æœ€å¤§è™•ç†æ•¸é‡
    
    è¿”å›:
    - åŒ…å«åŸºæœ¬é¢è³‡è¨Šçš„ DataFrame
    """
    print(f"[scraper] â³ é–‹å§‹æ“·å–æ³•äººèˆ‡æœ¬ç›Šæ¯”è³‡æ–™ (æœ€å¤šè™•ç† {max_stocks} æª”)...")
    base_url = "https://goodinfo.tw/tw/StockInfo.asp?STOCK_ID="
    
    # éš¨æ©ŸåŒ– User-Agent ä»¥é¿å…è¢«å°é–
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Safari/605.1.15",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:93.0) Gecko/20100101 Firefox/93.0",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Edg/94.0.992.38"
    ]
    
    headers = {
        "User-Agent": random.choice(user_agents),
        "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
        "Referer": "https://goodinfo.tw/tw/index.asp"
    }
    
    result = []
    
    # é™åˆ¶è™•ç†çš„æ•¸é‡
    if len(stock_ids) > max_stocks:
        print(f"[scraper] âš ï¸ é™åˆ¶è™•ç†æ•¸é‡ç‚º {max_stocks} æª”è‚¡ç¥¨ (åŸ {len(stock_ids)} æª”)")
        stock_ids = stock_ids[:max_stocks]

    # ä½¿ç”¨å„ªåŒ–çš„é‡è©¦è¨­ç½®ï¼Œä½†æ¸›å°‘é‡è©¦æ¬¡æ•¸
    session = create_retry_session(retries=1, backoff_factor=0.5)
    
    # ä¸¦è¡Œè™•ç†è‚¡ç¥¨ï¼Œä½†å¢åŠ é™æµæ§åˆ¶ï¼Œæ¸›å°‘ä¸¦è¡Œæ•¸
    with ThreadPoolExecutor(max_workers=2) as executor:
        futures = []
        
        for i, stock_id in enumerate(stock_ids):
            # æ¯2å€‹è«‹æ±‚æ·»åŠ å»¶é²ä»¥é¿å…éå¿«è«‹æ±‚
            if i > 0 and i % 2 == 0:
                time.sleep(BATCH_DELAY)
            
            futures.append(executor.submit(
                fetch_single_stock_fundamental, 
                stock_id, 
                session, 
                base_url, 
                headers
            ))
        
        # è™•ç†å®Œæˆçš„ä»»å‹™
        for future in as_completed(futures):
            try:
                data = future.result()
                if data:
                    result.append(data)
            except Exception as e:
                print(f"[scraper] âš ï¸ åŸºæœ¬é¢æ•¸æ“šç²å–ä»»å‹™å¤±æ•—: {e}")

    print(f"[scraper] âœ… æˆåŠŸç²å– {len(result)} æª”è‚¡ç¥¨çš„åŸºæœ¬é¢æ•¸æ“š")
    return pd.DataFrame(result)

def fetch_single_stock_fundamental(stock_id, session, base_url, headers):
    """ç²å–å–®ä¸€è‚¡ç¥¨çš„åŸºæœ¬é¢æ•¸æ“šï¼ŒåŒ…å«æ›´å¥½çš„éŒ¯èª¤è™•ç†"""
    try:
        stock_id = str(stock_id).replace('="', '').replace('"', '').strip()
        url = base_url + stock_id
        
        # å˜—è©¦é‡è©¦é‚è¼¯ï¼Œä½†ä½¿ç”¨æ›´çŸ­çš„è¶…æ™‚
        max_attempts = 1  # æ¸›å°‘é‡è©¦æ¬¡æ•¸ï¼Œæé«˜æ•ˆç‡
        for attempt in range(max_attempts):
            try:
                resp = session.get(url, headers=headers, timeout=(5, 10))
                if resp.status_code == 200:
                    break
                time.sleep(1)
            except Exception as e:
                if attempt < max_attempts - 1:
                    time.sleep(1)
                else:
                    raise
        
        soup = BeautifulSoup(resp.text, "html.parser")

        tables = pd.read_html(StringIO(str(soup)), flavor="bs4")
        summary_table = None
        for table in tables:
            if "æœ¬ç›Šæ¯”" in str(table):
                summary_table = table
                break

        if summary_table is None or len(summary_table.columns) < 2:
            raise ValueError("ç„¡æ³•æ“·å–æ­£ç¢ºæ¬„ä½")

        flat = summary_table.values.flatten()
        pe, pb, roe = None, None, None
        for idx, val in enumerate(flat):
            if str(val).strip() == "æœ¬ç›Šæ¯”":
                try:
                    pe = float(flat[idx + 1])
                except:
                    pe = None
            if str(val).strip() == "è‚¡åƒ¹æ·¨å€¼æ¯”":
                try:
                    pb = float(flat[idx + 1])
                except:
                    pb = None
            if str(val).strip() == "ROE":
                try:
                    roe = float(flat[idx + 1])
                except:
                    roe = None

        return {
            "è­‰åˆ¸ä»£è™Ÿ": stock_id,
            "PE": pe,
            "PB": pb,
            "ROE": roe,
            "å¤–è³‡": None,  # å¯æ“´å±•åŠ å…¥æ³•äººæŒè‚¡è³‡è¨Š
            "æŠ•ä¿¡": None,
            "è‡ªç‡Ÿå•†": None,
        }

    except Exception as e:
        print(f"[scraper] âš ï¸ {stock_id} æ“·å–å¤±æ•—ï¼š{e}")
        return None
