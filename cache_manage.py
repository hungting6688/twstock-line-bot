#!/usr/bin/env python3
"""
ç·©å­˜ç®¡ç†å·¥å…· - ç”¨æ–¼åˆ—å‡ºã€å‚™ä»½å’Œæ¸…ç†ç·©å­˜
2025å¹´å¢å¼·ç‰ˆ - æ”¯æŒå®šæœŸå‚™ä»½å’Œæ™ºèƒ½ç·©å­˜ç®¡ç†
"""

import os
import json
import glob
import time
import shutil
import argparse
from datetime import datetime, timedelta
import hashlib
import logging
import traceback
import threading
import sys
import zipfile

# è¨­ç½®ç·©å­˜ç›®éŒ„
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CACHE_DIR = os.path.join(BASE_DIR, 'cache')
BACKUP_DIR = os.path.join(BASE_DIR, 'cache_backup')
LOG_DIR = os.path.join(BASE_DIR, 'logs')

# ç¢ºä¿ç›®éŒ„å­˜åœ¨
for directory in [CACHE_DIR, BACKUP_DIR, LOG_DIR]:
    os.makedirs(directory, exist_ok=True)

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    filename=os.path.join(LOG_DIR, 'cache_manage.log'),
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# ç·©å­˜é¡å‹é…ç½® - ä¸åŒé¡å‹çš„ç·©å­˜æœ‰ä¸åŒçš„ä¿ç•™æœŸé™å’Œå‚™ä»½ç­–ç•¥
CACHE_CONFIG = {
    'eps_data_cache.json': {
        'description': 'EPS å’ŒåŸºæœ¬è²¡å‹™æ•¸æ“šç·©å­˜',
        'retention_days': 30,     # ä¿ç•™å¤©æ•¸
        'backup_interval': 7,     # å‚™ä»½é–“éš”ï¼ˆå¤©ï¼‰
        'backup_copies': 5,       # ä¿ç•™çš„å‚™ä»½å‰¯æœ¬æ•¸é‡
        'critical': True,         # æ˜¯å¦ç‚ºé—œéµç·©å­˜
        'auto_clean': False       # æ˜¯å¦è‡ªå‹•æ¸…ç†
    },
    'dividend_data_cache.json': {
        'description': 'è‚¡æ¯æ•¸æ“šç·©å­˜',
        'retention_days': 30,
        'backup_interval': 7,
        'backup_copies': 5,
        'critical': True,
        'auto_clean': False
    },
    'twse_stocks_cache.json': {
        'description': 'è‚¡ç¥¨åˆ—è¡¨ç·©å­˜',
        'retention_days': 90,
        'backup_interval': 30,
        'backup_copies': 3,
        'critical': True,
        'auto_clean': False
    },
    'multi_strategy_morning_cache.json': {
        'description': 'æ—©ç›¤æ¨è–¦ç·©å­˜',
        'retention_days': 7,
        'backup_interval': 1,
        'backup_copies': 7,
        'critical': False,
        'auto_clean': True
    },
    'multi_strategy_afternoon_cache.json': {
        'description': 'ä¸Šåˆçœ‹ç›¤æ¨è–¦ç·©å­˜',
        'retention_days': 3,
        'backup_interval': 1,
        'backup_copies': 3,
        'critical': False,
        'auto_clean': True
    },
    'multi_strategy_noon_cache.json': {
        'description': 'åˆç›¤æ¨è–¦ç·©å­˜',
        'retention_days': 3,
        'backup_interval': 1,
        'backup_copies': 3,
        'critical': False,
        'auto_clean': True
    },
    'multi_strategy_evening_cache.json': {
        'description': 'ç›¤å¾Œåˆ†æç·©å­˜',
        'retention_days': 7,
        'backup_interval': 1,
        'backup_copies': 7,
        'critical': False,
        'auto_clean': True
    }
}

def log_event(message, level='info'):
    """è¨˜éŒ„äº‹ä»¶åˆ°æ—¥èªŒ"""
    try:
        # è¼¸å‡ºåˆ°æ§åˆ¶å°
        if level == 'error':
            print(f"âŒ {message}")
            logging.error(message)
        elif level == 'warning':
            print(f"âš ï¸ {message}")
            logging.warning(message)
        else:
            print(f"â„¹ï¸ {message}")
            logging.info(message)
    except Exception as e:
        print(f"è¨˜éŒ„æ—¥èªŒå¤±æ•—: {e}")

def get_cache_info():
    """
    ç²å–ç·©å­˜æ–‡ä»¶çš„ä¿¡æ¯
    
    è¿”å›:
    - ç·©å­˜ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨ [{æ–‡ä»¶å, å¤§å°, æ™‚é–“æˆ³, å¹´é½¡}]
    """
    if not os.path.exists(CACHE_DIR):
        log_event(f"ç·©å­˜ç›®éŒ„ {CACHE_DIR} ä¸å­˜åœ¨", 'warning')
        return []
    
    cache_files = glob.glob(os.path.join(CACHE_DIR, '*.json'))
    if not cache_files:
        log_event("æ²’æœ‰æ‰¾åˆ°ç·©å­˜æ–‡ä»¶", 'warning')
        return []
    
    now = time.time()
    cache_info = []
    
    for file_path in cache_files:
        filename = os.path.basename(file_path)
        size = os.path.getsize(file_path)
        mtime = os.path.getmtime(file_path)
        age_seconds = now - mtime
        
        # å˜—è©¦å¾æ–‡ä»¶ä¸­è®€å–æ™‚é–“æˆ³å’Œå…§å®¹ç¸½çµ
        timestamp_str = "æœªçŸ¥"
        content_summary = "æœªçŸ¥"
        item_count = 0
        cache_type = "ä¸€èˆ¬ç·©å­˜"
        
        # ç²å–æª”æ¡ˆé…ç½®
        config = CACHE_CONFIG.get(filename, {
            "description": "ä¸€èˆ¬ç·©å­˜æ–‡ä»¶",
            "retention_days": 7,
            "backup_interval": 1,
            "backup_copies": 3,
            "critical": False,
            "auto_clean": True
        })
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                # å˜—è©¦ç²å–æ™‚é–“æˆ³
                if 'timestamp' in data:
                    timestamp_str = data['timestamp']
                
                # å˜—è©¦ç²å–å…§å®¹ç¸½çµ
                if 'data' in data and isinstance(data['data'], dict):
                    item_count = len(data['data'])
                    content_summary = f"{item_count} é …ç›®"
                elif 'recommendations' in data and isinstance(data['recommendations'], dict):
                    strategies = data['recommendations']
                    items = sum(len(stocks) for stocks in strategies.values())
                    content_summary = f"{items} æª”è‚¡ç¥¨æ¨è–¦"
        except Exception as e:
            log_event(f"è®€å–ç·©å­˜æ–‡ä»¶ {filename} å…§å®¹å¤±æ•—: {e}", 'warning')
        
        # è¨ˆç®— MD5 æ ¡é©—å’Œ
        try:
            with open(file_path, 'rb') as f:
                file_hash = hashlib.md5(f.read()).hexdigest()
        except Exception as e:
            file_hash = "è¨ˆç®—å¤±æ•—"
        
        cache_info.append({
            'filename': filename,
            'size': size,
            'mtime': mtime,
            'age_seconds': age_seconds,
            'timestamp': timestamp_str,
            'content_summary': content_summary,
            'item_count': item_count,
            'md5': file_hash,
            'description': config['description'],
            'retention_days': config['retention_days'],
            'critical': config['critical']
        })
    
    # æŒ‰ä¿®æ”¹æ™‚é–“æ’åº
    cache_info.sort(key=lambda x: x['mtime'], reverse=True)
    
    return cache_info

def list_cache(verbose=False):
    """
    åˆ—å‡ºæ‰€æœ‰ç·©å­˜æ–‡ä»¶
    
    åƒæ•¸:
    - verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°ä¿¡æ¯
    """
    cache_info = get_cache_info()
    
    if not cache_info:
        log_event("æ²’æœ‰æ‰¾åˆ°ç·©å­˜æ–‡ä»¶", 'warning')
        return
    
    print("\nç·©å­˜æ–‡ä»¶åˆ—è¡¨:")
    print("-" * 100)
    
    if verbose:
        print(f"{'æ–‡ä»¶å':<30} {'å¤§å°(KB)':<10} {'ä¿®æ”¹æ™‚é–“':<20} {'å¹´é½¡':<15} {'é …ç›®æ•¸':<10} {'æè¿°':<30}")
        print("-" * 100)
        
        for info in cache_info:
            mtime_str = datetime.fromtimestamp(info['mtime']).strftime('%Y-%m-%d %H:%M:%S')
            age_days = info['age_seconds'] / (24 * 3600)
            
            if age_days < 1:
                age_str = f"{int(info['age_seconds'] / 3600)}å°æ™‚"
            else:
                age_str = f"{age_days:.1f}å¤©"
            
            # å¢åŠ æ–‡ä»¶ç‹€æ…‹æŒ‡ç¤º
            is_critical = "ğŸ”’" if info['critical'] else ""
            
            print(f"{info['filename']:<30} {info['size']/1024:<10.1f} {mtime_str:<20} {age_str:<15} {info['item_count']:<10} {info['description']}{is_critical}")
    else:
        print(f"{'æ–‡ä»¶å':<30} {'å¤§å°(KB)':<10} {'ä¿®æ”¹æ™‚é–“':<20} {'å¹´é½¡':<15}")
        print("-" * 80)
        
        for info in cache_info:
            mtime_str = datetime.fromtimestamp(info['mtime']).strftime('%Y-%m-%d %H:%M:%S')
            age_days = info['age_seconds'] / (24 * 3600)
            
            if age_days < 1:
                age_str = f"{int(info['age_seconds'] / 3600)}å°æ™‚"
            else:
                age_str = f"{age_days:.1f}å¤©"
            
            print(f"{info['filename']:<30} {info['size']/1024:<10.1f} {mtime_str:<20} {age_str:<15}")
    
    # è¨ˆç®—ç¸½ä½”ç”¨ç©ºé–“
    total_size = sum(info['size'] for info in cache_info)
    print(f"\nç¸½è¨ˆ: {len(cache_info)} å€‹ç·©å­˜æ–‡ä»¶ï¼Œç¸½å…±ä½”ç”¨ {total_size/1024/1024:.2f} MB ç£ç¢Ÿç©ºé–“")

def backup_cache(specific_file=None):
    """
    å‚™ä»½ç·©å­˜æ–‡ä»¶
    
    åƒæ•¸:
    - specific_file: æŒ‡å®šè¦å‚™ä»½çš„æ–‡ä»¶åï¼ŒNoneè¡¨ç¤ºå‚™ä»½æ‰€æœ‰æ–‡ä»¶
    
    è¿”å›:
    - bool: æ˜¯å¦æˆåŠŸå‚™ä»½
    """
    if not os.path.exists(CACHE_DIR):
        log_event(f"ç·©å­˜ç›®éŒ„ {CACHE_DIR} ä¸å­˜åœ¨ï¼Œç„¡æ³•å‚™ä»½", 'error')
        return False
    
    # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶ï¼Œæª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if specific_file:
        file_path = os.path.join(CACHE_DIR, specific_file)
        if not os.path.exists(file_path):
            log_event(f"æŒ‡å®šçš„æ–‡ä»¶ {specific_file} ä¸å­˜åœ¨ï¼Œç„¡æ³•å‚™ä»½", 'error')
            return False
        cache_files = [file_path]
    else:
        cache_files = glob.glob(os.path.join(CACHE_DIR, '*.json'))
    
    if not cache_files:
        log_event("æ²’æœ‰æ‰¾åˆ°ç·©å­˜æ–‡ä»¶ï¼Œç„¡éœ€å‚™ä»½", 'warning')
        return False
    
    # å‰µå»ºå‚™ä»½ç›®éŒ„
    os.makedirs(BACKUP_DIR, exist_ok=True)
    
    # å‰µå»ºå¸¶æ™‚é–“æˆ³çš„å‚™ä»½å­ç›®éŒ„
    backup_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_subdir = os.path.join(BACKUP_DIR, f'backup_{backup_timestamp}')
    os.makedirs(backup_subdir, exist_ok=True)
    
    # å‚™ä»½æ–‡ä»¶
    success_count = 0
    backup_details = []
    
    for file_path in cache_files:
        try:
            filename = os.path.basename(file_path)
            
            # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦ç‚ºç©ºæˆ–æå£
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                log_event(f"è·³éç©ºæ–‡ä»¶ {filename}", 'warning')
                continue
                
            try:
                # å˜—è©¦è®€å–æ–‡ä»¶ç¢ºèªæ˜¯å¦ç‚ºæœ‰æ•ˆçš„JSON
                with open(file_path, 'r', encoding='utf-8') as f:
                    json.load(f)
            except json.JSONDecodeError:
                log_event(f"è·³éç„¡æ•ˆçš„JSONæ–‡ä»¶ {filename}", 'warning')
                continue
            
            # è¤‡è£½æ–‡ä»¶åˆ°å‚™ä»½ç›®éŒ„
            dest_path = os.path.join(backup_subdir, filename)
            shutil.copy2(file_path, dest_path)
            
            # è¨ˆç®—å‚™ä»½æ–‡ä»¶çš„MD5æ ¡é©—å’Œ
            with open(dest_path, 'rb') as f:
                file_hash = hashlib.md5(f.read()).hexdigest()
            
            # è¨˜éŒ„å‚™ä»½è©³æƒ…
            backup_details.append({
                'filename': filename,
                'original_path': file_path,
                'backup_path': dest_path,
                'size': file_size,
                'md5': file_hash,
                'backup_time': datetime.now().isoformat()
            })
            
            success_count += 1
            log_event(f"æˆåŠŸå‚™ä»½ {filename}")
            
        except Exception as e:
            log_event(f"å‚™ä»½æ–‡ä»¶ {file_path} å¤±æ•—: {e}", 'error')
    
    # å‰µå»ºå‚™ä»½ç´¢å¼•æ–‡ä»¶
    if backup_details:
        try:
            index_file = os.path.join(backup_subdir, 'backup_index.json')
            with open(index_file, 'w', encoding='utf-8') as f:
                backup_index = {
                    'timestamp': datetime.now().isoformat(),
                    'backup_id': backup_timestamp,
                    'files': backup_details
                }
                json.dump(backup_index, f, ensure_ascii=False, indent=2)
        except Exception as e:
            log_event(f"å‰µå»ºå‚™ä»½ç´¢å¼•å¤±æ•—: {e}", 'warning')
    
    # å»ºç«‹å£“ç¸®å‚™ä»½
    try:
        zip_file = os.path.join(BACKUP_DIR, f'backup_{backup_timestamp}.zip')
        with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(backup_subdir):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, backup_subdir)
                    zipf.write(file_path, arcname)
        log_event(f"å·²å‰µå»ºå£“ç¸®å‚™ä»½: {zip_file}")
    except Exception as e:
        log_event(f"å‰µå»ºå£“ç¸®å‚™ä»½å¤±æ•—: {e}", 'warning')
    
    log_event(f"æˆåŠŸå‚™ä»½ {success_count}/{len(cache_files)} å€‹ç·©å­˜æ–‡ä»¶åˆ° {backup_subdir}")
    return success_count > 0

def clean_old_cache(days=None, force=False):
    """
    æ¸…ç†éèˆŠçš„ç·©å­˜æ–‡ä»¶
    
    åƒæ•¸:
    - days: ä¿ç•™çš„å¤©æ•¸ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æ¯å€‹æ–‡ä»¶çš„é…ç½®
    - force: æ˜¯å¦å¼·åˆ¶æ¸…ç†ï¼ˆåŒ…æ‹¬é—œéµç·©å­˜ï¼‰
    
    è¿”å›:
    - int: åˆªé™¤çš„æ–‡ä»¶æ•¸é‡
    """
    if not os.path.exists(CACHE_DIR):
        log_event(f"ç·©å­˜ç›®éŒ„ {CACHE_DIR} ä¸å­˜åœ¨ï¼Œç„¡éœ€æ¸…ç†", 'warning')
        return 0
    
    now = time.time()
    cache_files = glob.glob(os.path.join(CACHE_DIR, '*.json'))
    
    deleted_count = 0
    
    for file_path in cache_files:
        filename = os.path.basename(file_path)
        
        # ç²å–æ–‡ä»¶é…ç½®
        config = CACHE_CONFIG.get(filename, {
            "description": "ä¸€èˆ¬ç·©å­˜æ–‡ä»¶",
            "retention_days": 7,
            "critical": False,
            "auto_clean": True
        })
        
        # å¦‚æœæ–‡ä»¶æ˜¯é—œéµç·©å­˜ä¸”ä¸å¼·åˆ¶æ¸…ç†ï¼Œå‰‡è·³é
        if config['critical'] and not force:
            log_event(f"è·³éé—œéµç·©å­˜æ–‡ä»¶: {filename}")
            continue
        
        # ç¢ºå®šä¿ç•™å¤©æ•¸
        retention_days = days if days is not None else config['retention_days']
        
        # æª¢æŸ¥æ–‡ä»¶å¹´é½¡
        mtime = os.path.getmtime(file_path)
        age_days = (now - mtime) / (24 * 3600)
        
        if age_days > retention_days:
            try:
                # åœ¨åˆªé™¤å‰å…ˆé€²è¡Œä¸€æ¬¡å‚™ä»½
                backup_single_file(filename)
                
                # åˆªé™¤æ–‡ä»¶
                os.remove(file_path)
                deleted_count += 1
                log_event(f"å·²åˆªé™¤éæœŸç·©å­˜: {filename} (å¹´é½¡: {age_days:.1f}å¤©)")
            except Exception as e:
                log_event(f"åˆªé™¤æ–‡ä»¶ {file_path} å¤±æ•—: {e}", 'error')
    
    log_event(f"æ¸…ç†å®Œæˆï¼Œå…±åˆªé™¤ {deleted_count} å€‹éæœŸç·©å­˜æ–‡ä»¶")
    return deleted_count

def backup_single_file(filename):
    """
    å‚™ä»½å–®å€‹ç·©å­˜æ–‡ä»¶
    
    åƒæ•¸:
    - filename: æ–‡ä»¶å
    
    è¿”å›:
    - bool: æ˜¯å¦æˆåŠŸå‚™ä»½
    """
    try:
        file_path = os.path.join(CACHE_DIR, filename)
        if not os.path.exists(file_path):
            log_event(f"æ–‡ä»¶ {filename} ä¸å­˜åœ¨ï¼Œç„¡æ³•å‚™ä»½", 'warning')
            return False
        
        # å‰µå»ºå‚™ä»½ç›®éŒ„
        file_backup_dir = os.path.join(BACKUP_DIR, filename.replace('.json', ''))
        os.makedirs(file_backup_dir, exist_ok=True)
        
        # å‰µå»ºå¸¶æ™‚é–“æˆ³çš„å‚™ä»½æ–‡ä»¶å
        backup_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = os.path.join(file_backup_dir, f"{filename.replace('.json', '')}_{backup_timestamp}.json")
        
        # è¤‡è£½æ–‡ä»¶
        shutil.copy2(file_path, backup_file)
        log_event(f"å·²å‚™ä»½ {filename} åˆ° {backup_file}")
        
        # ç¶­è­·å‚™ä»½çš„ç‰ˆæœ¬æ•¸é‡
        config = CACHE_CONFIG.get(filename, {
            "backup_copies": 3
        })
        max_copies = config.get("backup_copies", 3)
        
        # ç²å–è©²æ–‡ä»¶çš„æ‰€æœ‰å‚™ä»½
        backups = glob.glob(os.path.join(file_backup_dir, f"{filename.replace('.json', '')}*.json"))
        backups.sort(key=os.path.getmtime, reverse=True)
        
        # å¦‚æœå‚™ä»½æ•¸é‡è¶…éé™åˆ¶ï¼Œåˆªé™¤æœ€èˆŠçš„å‚™ä»½
        if len(backups) > max_copies:
            for old_backup in backups[max_copies:]:
                os.remove(old_backup)
                log_event(f"å·²åˆªé™¤éèˆŠçš„å‚™ä»½: {os.path.basename(old_backup)}")
        
        return True
    except Exception as e:
        log_event(f"å‚™ä»½æ–‡ä»¶ {filename} å¤±æ•—: {e}", 'error')
        return False

def run_system_check():
    """é‹è¡Œç³»çµ±å¥åº·æª¢æŸ¥"""
    log_event("é–‹å§‹ç³»çµ±å¥åº·æª¢æŸ¥...")
    
    # 1. æª¢æŸ¥ç·©å­˜ç›®éŒ„æ˜¯å¦å­˜åœ¨ä¸¦ä¸”å¯è®€å¯«
    log_event("\n1. ç·©å­˜ç›®éŒ„æª¢æŸ¥:")
    if not os.path.exists(CACHE_DIR):
        log_event(f"ç·©å­˜ç›®éŒ„ {CACHE_DIR} ä¸å­˜åœ¨", 'warning')
        try:
            os.makedirs(CACHE_DIR)
            log_event(f"å·²å‰µå»ºç·©å­˜ç›®éŒ„ {CACHE_DIR}")
        except Exception as e:
            log_event(f"å‰µå»ºç·©å­˜ç›®éŒ„å¤±æ•—: {e}", 'error')
    else:
        log_event(f"ç·©å­˜ç›®éŒ„ {CACHE_DIR} å­˜åœ¨")
        
        # æ¸¬è©¦å¯«å…¥æ¬Šé™
        test_file = os.path.join(CACHE_DIR, 'test_write.tmp')
        try:
            with open(test_file, 'w') as f:
                f.write('test')
            os.remove(test_file)
            log_event(f"ç·©å­˜ç›®éŒ„å¯è®€å¯«")
        except Exception as e:
            log_event(f"ç·©å­˜ç›®éŒ„å¯«å…¥æ¸¬è©¦å¤±æ•—: {e}", 'error')
    
    # 2. æª¢æŸ¥å‚™ä»½ç›®éŒ„
    log_event("\n2. å‚™ä»½ç›®éŒ„æª¢æŸ¥:")
    if not os.path.exists(BACKUP_DIR):
        log_event(f"å‚™ä»½ç›®éŒ„ {BACKUP_DIR} ä¸å­˜åœ¨", 'warning')
        try:
            os.makedirs(BACKUP_DIR)
            log_event(f"å·²å‰µå»ºå‚™ä»½ç›®éŒ„ {BACKUP_DIR}")
        except Exception as e:
            log_event(f"å‰µå»ºå‚™ä»½ç›®éŒ„å¤±æ•—: {e}", 'error')
    else:
        log_event(f"å‚™ä»½ç›®éŒ„ {BACKUP_DIR} å­˜åœ¨")
        
        # æ¸¬è©¦å¯«å…¥æ¬Šé™
        test_file = os.path.join(BACKUP_DIR, 'test_write.tmp')
        try:
            with open(test_file, 'w') as f:
                f.write('test')
            os.remove(test_file)
            log_event(f"å‚™ä»½ç›®éŒ„å¯è®€å¯«")
        except Exception as e:
            log_event(f"å‚™ä»½ç›®éŒ„å¯«å…¥æ¸¬è©¦å¤±æ•—: {e}", 'error')
    
    # 3. æª¢æŸ¥ç·©å­˜æ–‡ä»¶
    log_event("\n3. ç·©å­˜æ–‡ä»¶æª¢æŸ¥:")
    cache_info = get_cache_info()
    if not cache_info:
        log_event("æ²’æœ‰æ‰¾åˆ°ç·©å­˜æ–‡ä»¶", 'warning')
    else:
        log_event(f"æ‰¾åˆ° {len(cache_info)} å€‹ç·©å­˜æ–‡ä»¶")
        
        # æª¢æŸ¥é—œéµç·©å­˜æ–‡ä»¶
        critical_files = [name for name, config in CACHE_CONFIG.items() if config.get('critical', False)]
        for filename in critical_files:
            file_path = os.path.join(CACHE_DIR, filename)
            if os.path.exists(file_path):
                # æª¢æŸ¥æ–‡ä»¶å…§å®¹æ˜¯å¦ç‚ºæœ‰æ•ˆçš„JSON
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    log_event(f"é—œéµç·©å­˜ {filename} å­˜åœ¨ä¸”æœ‰æ•ˆ")
                except Exception as e:
                    log_event(f"é—œéµç·©å­˜ {filename} å…§å®¹ç„¡æ•ˆ: {e}", 'error')
            else:
                log_event(f"é—œéµç·©å­˜ {filename} ä¸å­˜åœ¨", 'warning')
    
    # 4. æª¢æŸ¥ç£ç›¤ç©ºé–“
    log_event("\n4. ç£ç›¤ç©ºé–“æª¢æŸ¥:")
    try:
        import shutil
        total, used, free = shutil.disk_usage(BASE_DIR)
        free_gb = free / (1024**3)
        used_percent = used / total * 100
        
        log_event(f"ç¸½ç£ç›¤ç©ºé–“: {total / (1024**3):.2f} GB")
        log_event(f"å·²ä½¿ç”¨ç©ºé–“: {used / (1024**3):.2f} GB ({used_percent:.1f}%)")
        log_event(f"å‰©é¤˜ç©ºé–“: {free_gb:.2f} GB")
        
        if free_gb < 1:
            log_event(f"ç£ç›¤ç©ºé–“ä¸è¶³ï¼Œåƒ…å‰© {free_gb:.2f} GB", 'warning')
    except Exception as e:
        log_event(f"æª¢æŸ¥ç£ç›¤ç©ºé–“å¤±æ•—: {e}", 'error')
    
    log_event("\nç³»çµ±å¥åº·æª¢æŸ¥å®Œæˆ")

def auto_cleanup_backups():
    """
    è‡ªå‹•æ¸…ç†éèˆŠçš„å‚™ä»½
    
    è¿”å›:
    - int: åˆªé™¤çš„å‚™ä»½æ•¸é‡
    """
    if not os.path.exists(BACKUP_DIR):
        log_event(f"å‚™ä»½ç›®éŒ„ {BACKUP_DIR} ä¸å­˜åœ¨ï¼Œç„¡éœ€æ¸…ç†", 'warning')
        return 0
    
    # ç²å–æ‰€æœ‰å‚™ä»½å­ç›®éŒ„
    backup_dirs = [d for d in os.listdir(BACKUP_DIR) 
                   if os.path.isdir(os.path.join(BACKUP_DIR, d)) and d.startswith('backup_')]
    
    # ç²å–æ‰€æœ‰å£“ç¸®å‚™ä»½
    backup_zips = [f for f in os.listdir(BACKUP_DIR) 
                   if os.path.isfile(os.path.join(BACKUP_DIR, f)) and f.startswith('backup_') and f.endswith('.zip')]
    
    # ä¿ç•™çš„å‚™ä»½æ•¸é‡
    max_backups = 10
    
    # åˆªé™¤éèˆŠçš„å­ç›®éŒ„å‚™ä»½
    deleted_dirs = 0
    if len(backup_dirs) > max_backups:
        # æŒ‰æ™‚é–“æ’åº
        backup_dirs.sort(key=lambda d: os.path.getmtime(os.path.join(BACKUP_DIR, d)), reverse=True)
        
        # åˆªé™¤å¤šé¤˜çš„å‚™ä»½
        for old_dir in backup_dirs[max_backups:]:
            dir_path = os.path.join(BACKUP_DIR, old_dir)
            try:
                shutil.rmtree(dir_path)
                deleted_dirs += 1
                log_event(f"å·²åˆªé™¤éèˆŠçš„å‚™ä»½ç›®éŒ„: {old_dir}")
            except Exception as e:
                log_event(f"åˆªé™¤å‚™ä»½ç›®éŒ„ {old_dir} å¤±æ•—: {e}", 'error')
    
    # åˆªé™¤éèˆŠçš„å£“ç¸®å‚™ä»½
    deleted_zips = 0
    if len(backup_zips) > max_backups:
        # æŒ‰æ™‚é–“æ’åº
        backup_zips.sort(key=lambda f: os.path.getmtime(os.path.join(BACKUP_DIR, f)), reverse=True)
        
        # åˆªé™¤å¤šé¤˜çš„å‚™ä»½
        for old_zip in backup_zips[max_backups:]:
            zip_path = os.path.join(BACKUP_DIR, old_zip)
            try:
                os.remove(zip_path)
                deleted_zips += 1
                log_event(f"å·²åˆªé™¤éèˆŠçš„å£“ç¸®å‚™ä»½: {old_zip}")
            except Exception as e:
                log_event(f"åˆªé™¤å£“ç¸®å‚™ä»½ {old_zip} å¤±æ•—: {e}", 'error')
    
    log_event(f"æ¸…ç†å®Œæˆï¼Œå…±åˆªé™¤ {deleted_dirs} å€‹å‚™ä»½ç›®éŒ„å’Œ {deleted_zips} å€‹å£“ç¸®å‚™ä»½")
    return deleted_dirs + deleted_zips

def restore_backup(backup_id=None, filename=None):
    """
    å¾å‚™ä»½æ¢å¾©ç·©å­˜æ–‡ä»¶
    
    åƒæ•¸:
    - backup_id: å‚™ä»½IDï¼ˆæ™‚é–“æˆ³ï¼‰ï¼ŒNoneè¡¨ç¤ºæœ€æ–°çš„å‚™ä»½
    - filename: è¦æ¢å¾©çš„æ–‡ä»¶åï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰æ–‡ä»¶
    
    è¿”å›:
    - bool: æ˜¯å¦æˆåŠŸæ¢å¾©
    """
    if not os.path.exists(BACKUP_DIR):
        log_event(f"å‚™ä»½ç›®éŒ„ {BACKUP_DIR} ä¸å­˜åœ¨ï¼Œç„¡æ³•æ¢å¾©", 'error')
        return False
    
    # æŸ¥æ‰¾å‚™ä»½ç›®éŒ„
    backup_dirs = [d for d in os.listdir(BACKUP_DIR) 
                  if os.path.isdir(os.path.join(BACKUP_DIR, d)) and d.startswith('backup_')]
    
    if not backup_dirs:
        log_event("æ²’æœ‰æ‰¾åˆ°å‚™ä»½ç›®éŒ„", 'error')
        return False
    
    # å¦‚æœæ²’æœ‰æŒ‡å®šå‚™ä»½IDï¼Œä½¿ç”¨æœ€æ–°çš„å‚™ä»½
    if backup_id is None:
        backup_dirs.sort(key=lambda d: os.path.getmtime(os.path.join(BACKUP_DIR, d)), reverse=True)
        backup_id = backup_dirs[0].replace('backup_', '')
    
    backup_dir = os.path.join(BACKUP_DIR, f'backup_{backup_id}')
    if not os.path.exists(backup_dir):
        log_event(f"æŒ‡å®šçš„å‚™ä»½ç›®éŒ„ {backup_dir} ä¸å­˜åœ¨", 'error')
        return False
    
    # ç²å–å‚™ä»½ç´¢å¼•æ–‡ä»¶
    index_file = os.path.join(backup_dir, 'backup_index.json')
    if os.path.exists(index_file):
        try:
            with open(index_file, 'r', encoding='utf-8') as f:
                backup_index = json.load(f)
            log_event(f"è®€å–å‚™ä»½ç´¢å¼•æˆåŠŸï¼Œå‚™ä»½æ–¼ {backup_index.get('timestamp', 'æœªçŸ¥æ™‚é–“')}")
        except Exception as e:
            log_event(f"è®€å–å‚™ä»½ç´¢å¼•å¤±æ•—: {e}", 'warning')
            backup_index = None
    else:
        backup_index = None
    
    # ç²å–å‚™ä»½æ–‡ä»¶åˆ—è¡¨
    if filename:
        backup_files = [os.path.join(backup_dir, filename)]
        if not os.path.exists(backup_files[0]):
            log_event(f"æŒ‡å®šçš„å‚™ä»½æ–‡ä»¶ {filename} ä¸å­˜åœ¨æ–¼å‚™ä»½ {backup_id}", 'error')
            return False
    else:
        backup_files = [os.path.join(backup_dir, f) for f in os.listdir(backup_dir) 
                        if os.path.isfile(os.path.join(backup_dir, f)) and f.endswith('.json') and f != 'backup_index.json']
    
    if not backup_files:
        log_event(f"å‚™ä»½ {backup_id} ä¸­æ²’æœ‰æ‰¾åˆ°ç·©å­˜æ–‡ä»¶", 'error')
        return False
    
    # æ¢å¾©å‰å…ˆå‚™ä»½ç•¶å‰ç·©å­˜
    current_backup_id = datetime.now().strftime('%Y%m%d_%H%M%S')
    current_backup_dir = os.path.join(BACKUP_DIR, f'pre_restore_{current_backup_id}')
    os.makedirs(current_backup_dir, exist_ok=True)
    
    # å‚™ä»½ç•¶å‰ç·©å­˜æ–‡ä»¶
    current_files = glob.glob(os.path.join(CACHE_DIR, '*.json'))
    for file_path in current_files:
        filename = os.path.basename(file_path)
        try:
            shutil.copy2(file_path, os.path.join(current_backup_dir, filename))
            log_event(f"å·²å‚™ä»½ç•¶å‰ç·©å­˜æ–‡ä»¶ {filename}")
        except Exception as e:
            log_event(f"å‚™ä»½ç•¶å‰ç·©å­˜æ–‡ä»¶ {filename} å¤±æ•—: {e}", 'warning')
    
    # é–‹å§‹æ¢å¾©
    success_count = 0
    for backup_file in backup_files:
        try:
            filename = os.path.basename(backup_file)
            dest_file = os.path.join(CACHE_DIR, filename)
            
            # è¤‡è£½å‚™ä»½æ–‡ä»¶åˆ°ç·©å­˜ç›®éŒ„
            shutil.copy2(backup_file, dest_file)
            success_count += 1
            log_event(f"æˆåŠŸæ¢å¾© {filename}")
        except Exception as e:
            log_event(f"æ¢å¾©æ–‡ä»¶ {os.path.basename(backup_file)} å¤±æ•—: {e}", 'error')
    
    log_event(f"æ¢å¾©å®Œæˆï¼Œå…±æ¢å¾© {success_count}/{len(backup_files)} å€‹ç·©å­˜æ–‡ä»¶")
    return success_count > 0

def automatic_backup():
    """
    è‡ªå‹•å‚™ä»½ç·©å­˜æ–‡ä»¶
    
    è¿”å›:
    - bool: æ˜¯å¦æœ‰æ–‡ä»¶éœ€è¦å‚™ä»½
    """
    cache_info = get_cache_info()
    
    if not cache_info:
        log_event("æ²’æœ‰æ‰¾åˆ°ç·©å­˜æ–‡ä»¶ï¼Œç„¡éœ€å‚™ä»½", 'warning')
        return False
    
    files_to_backup = []
    
    # æª¢æŸ¥æ¯å€‹ç·©å­˜æ–‡ä»¶
    for info in cache_info:
        filename = info['filename']
        config = CACHE_CONFIG.get(filename, {
            "backup_interval": 7,
            "critical": False
        })
        
        # ç²å–ä¸Šæ¬¡å‚™ä»½æ™‚é–“
        file_backup_dir = os.path.join(BACKUP_DIR, filename.replace('.json', ''))
        if os.path.exists(file_backup_dir):
            backups = glob.glob(os.path.join(file_backup_dir, f"{filename.replace('.json', '')}*.json"))
            if backups:
                backups.sort(key=os.path.getmtime, reverse=True)
                last_backup_time = os.path.getmtime(backups[0])
                age_days = (time.time() - last_backup_time) / (24 * 3600)
                
                # æª¢æŸ¥æ˜¯å¦éœ€è¦å‚™ä»½
                if age_days >= config['backup_interval']:
                    files_to_backup.append(filename)
            else:
                # æ²’æœ‰å‚™ä»½ï¼Œéœ€è¦å‚™ä»½
                files_to_backup.append(filename)
        else:
            # å‚™ä»½ç›®éŒ„ä¸å­˜åœ¨ï¼Œéœ€è¦å‚™ä»½
            files_to_backup.append(filename)
    
    # åŸ·è¡Œå‚™ä»½
    if files_to_backup:
        log_event(f"å°‡è‡ªå‹•å‚™ä»½ {len(files_to_backup)} å€‹ç·©å­˜æ–‡ä»¶")
        for filename in files_to_backup:
            backup_single_file(filename)
        return True
    else:
        log_event("æ‰€æœ‰ç·©å­˜æ–‡ä»¶éƒ½åœ¨å‚™ä»½å‘¨æœŸå…§ï¼Œä¸éœ€è¦è‡ªå‹•å‚™ä»½")
        return False

def cache_health_check():
    """
    åŸ·è¡Œç·©å­˜å¥åº·æª¢æŸ¥
    
    è¿”å›:
    - dict: ç·©å­˜å¥åº·ç‹€æ…‹å ±å‘Š
    """
    cache_info = get_cache_info()
    
    if not cache_info:
        return {
            "status": "warning",
            "message": "æ²’æœ‰æ‰¾åˆ°ç·©å­˜æ–‡ä»¶",
            "timestamp": datetime.now().isoformat(),
            "details": {}
        }
    
    # åˆå§‹åŒ–å¥åº·å ±å‘Š
    health_report = {
        "status": "ok",
        "message": "ç·©å­˜ç‹€æ…‹æ­£å¸¸",
        "timestamp": datetime.now().isoformat(),
        "cache_count": len(cache_info),
        "total_size_kb": sum(info['size'] for info in cache_info) / 1024,
        "critical_cache": {},
        "warnings": [],
        "details": {}
    }
    
    # æª¢æŸ¥æ¯å€‹ç·©å­˜æ–‡ä»¶
    for info in cache_info:
        filename = info['filename']
        config = CACHE_CONFIG.get(filename, {
            "description": "ä¸€èˆ¬ç·©å­˜æ–‡ä»¶",
            "retention_days": 7,
            "critical": False
        })
        
        # åŸºæœ¬ä¿¡æ¯
        file_health = {
            "file_size_kb": info['size'] / 1024,
            "modified": datetime.fromtimestamp(info['mtime']).isoformat(),
            "age_days": info['age_seconds'] / (24 * 3600),
            "critical": config['critical'],
            "item_count": info['item_count'],
            "status": "ok"
        }
        
        # æª¢æŸ¥æ–‡ä»¶çš„å¥åº·ç‹€æ…‹
        if config['critical']:
            # æª¢æŸ¥é—œéµç·©å­˜
            health_report["critical_cache"][filename] = file_health
            
            # æª¢æŸ¥æ˜¯å¦éæœŸ
            if file_health["age_days"] > config['retention_days'] * 2:
                file_health["status"] = "error"
                file_health["message"] = f"é—œéµç·©å­˜ {filename} å·²éæœŸ {file_health['age_days']:.1f} å¤©"
                health_report["warnings"].append(file_health["message"])
                health_report["status"] = "error"
            elif file_health["age_days"] > config['retention_days']:
                file_health["status"] = "warning"
                file_health["message"] = f"é—œéµç·©å­˜ {filename} æ¥è¿‘éæœŸ {file_health['age_days']:.1f} å¤©"
                health_report["warnings"].append(file_health["message"])
                if health_report["status"] != "error":
                    health_report["status"] = "warning"
        
        # æª¢æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦ç•°å¸¸
        if info['size'] < 10:
            file_health["status"] = "error"
            file_health["message"] = f"ç·©å­˜æ–‡ä»¶ {filename} å¯èƒ½ç‚ºç©ºæˆ–æå£ ({info['size']} å­—ç¯€)"
            health_report["warnings"].append(file_health["message"])
            health_report["status"] = "error"
        
        # æª¢æŸ¥é …ç›®æ•¸é‡æ˜¯å¦ç•°å¸¸
        if config['critical'] and info['item_count'] < 5:
            file_health["status"] = "warning"
            file_health["message"] = f"é—œéµç·©å­˜ {filename} é …ç›®æ•¸é‡ç•°å¸¸ (åƒ… {info['item_count']} é …)"
            health_report["warnings"].append(file_health["message"])
            if health_report["status"] != "error":
                health_report["status"] = "warning"
        
        # æ·»åŠ åˆ°è©³ç´°ä¿¡æ¯
        health_report["details"][filename] = file_health
    
    # æª¢æŸ¥æ˜¯å¦ç¼ºå°‘é—œéµç·©å­˜
    for filename, config in CACHE_CONFIG.items():
        if config.get('critical', False) and filename not in health_report["critical_cache"]:
            warning = f"ç¼ºå°‘é—œéµç·©å­˜ {filename}"
            health_report["warnings"].append(warning)
            health_report["status"] = "error"
    
    # æ›´æ–°ç¸½é«”ç‹€æ…‹æ¶ˆæ¯
    if health_report["status"] == "error":
        health_report["message"] = f"ç·©å­˜å­˜åœ¨åš´é‡å•é¡Œ ({len(health_report['warnings'])} å€‹è­¦å‘Š)"
    elif health_report["status"] == "warning":
        health_report["message"] = f"ç·©å­˜å­˜åœ¨æ½›åœ¨å•é¡Œ ({len(health_report['warnings'])} å€‹è­¦å‘Š)"
    
    return health_report

def print_cache_health(report=None):
    """
    æ‰“å°ç·©å­˜å¥åº·å ±å‘Š
    
    åƒæ•¸:
    - report: ç·©å­˜å¥åº·å ±å‘Šå­—å…¸ï¼ŒNoneè¡¨ç¤ºé‡æ–°ç²å–
    """
    if report is None:
        report = cache_health_check()
    
    # æ‰“å°å ±å‘Šé ­
    status_icon = "âœ…" if report["status"] == "ok" else "âš ï¸" if report["status"] == "warning" else "âŒ"
    print(f"\n{status_icon} ç·©å­˜å¥åº·å ±å‘Š - {report['message']}")
    print(f"ç”Ÿæˆæ™‚é–“: {datetime.fromisoformat(report['timestamp']).strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ç·©å­˜æ–‡ä»¶ç¸½æ•¸: {report['cache_count']} | ç¸½å¤§å°: {report['total_size_kb']:.1f} KB")
    print("-" * 80)
    
    # æ‰“å°è­¦å‘Šä¿¡æ¯
    if report["warnings"]:
        print("\nâš ï¸ è­¦å‘Š:")
        for warning in report["warnings"]:
            print(f"  - {warning}")
        print("")
    
    # æ‰“å°é—œéµç·©å­˜ç‹€æ…‹
    print("ğŸ”’ é—œéµç·©å­˜ç‹€æ…‹:")
    if report["critical_cache"]:
        for filename, info in report["critical_cache"].items():
            status = "âœ…" if info["status"] == "ok" else "âš ï¸" if info["status"] == "warning" else "âŒ"
            print(f"  {status} {filename} - å¹´é½¡: {info['age_days']:.1f}å¤©, å¤§å°: {info['file_size_kb']:.1f}KB, é …ç›®æ•¸: {info['item_count']}")
    else:
        print("  æ²’æœ‰æ‰¾åˆ°é—œéµç·©å­˜æ–‡ä»¶")
    
    # æ‰“å°ç¼ºå¤±çš„é—œéµç·©å­˜
    missing_critical = []
    for filename, config in CACHE_CONFIG.items():
        if config.get('critical', False) and filename not in report["critical_cache"]:
            missing_critical.append(filename)
    
    if missing_critical:
        print("\nâŒ ç¼ºå¤±çš„é—œéµç·©å­˜:")
        for filename in missing_critical:
            print(f"  - {filename}")
    
    print("\nå»ºè­°æ“ä½œ:")
    if report["status"] == "ok":
        print("  ç³»çµ±é‹è¡Œæ­£å¸¸ï¼Œç„¡éœ€ç‰¹æ®Šæ“ä½œ")
    else:
        if "error" in report["status"]:
            print("  1. åŸ·è¡Œç·©å­˜å‚™ä»½: python cache_manage.py --backup")
            print("  2. æª¢æŸ¥ä¸¦æ¢å¾©ç¼ºå¤±çš„é—œéµç·©å­˜: python cache_manage.py --restore")
            print("  3. é‹è¡Œç³»çµ±å¥åº·æª¢æŸ¥: python cache_manage.py --check")
        else:
            print("  1. åŸ·è¡Œç·©å­˜å‚™ä»½: python cache_manage.py --backup")
            print("  2. æ¸…ç†éæœŸç·©å­˜: python cache_manage.py --clean-old")

def init_cache():
    """
    åˆå§‹åŒ–ç·©å­˜ç›®éŒ„å’Œçµæ§‹
    
    è¿”å›:
    - bool: æ˜¯å¦æˆåŠŸåˆå§‹åŒ–
    """
    try:
        # ç¢ºä¿ç·©å­˜ç›®éŒ„å­˜åœ¨
        os.makedirs(CACHE_DIR, exist_ok=True)
        
        # ç¢ºä¿å‚™ä»½ç›®éŒ„å­˜åœ¨
        os.makedirs(BACKUP_DIR, exist_ok=True)
        
        # ç¢ºä¿æ—¥èªŒç›®éŒ„å­˜åœ¨
        os.makedirs(LOG_DIR, exist_ok=True)
        
        # å‰µå»ºç·©å­˜é…ç½®æ–‡ä»¶
        config_file = os.path.join(CACHE_DIR, 'cache_config.json')
        if not os.path.exists(config_file):
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(CACHE_CONFIG, f, ensure_ascii=False, indent=2)
            log_event(f"å·²å‰µå»ºç·©å­˜é…ç½®æ–‡ä»¶: {config_file}")
        
        log_event("æˆåŠŸåˆå§‹åŒ–ç·©å­˜ç³»çµ±")
        return True
    except Exception as e:
        log_event(f"åˆå§‹åŒ–ç·©å­˜ç³»çµ±å¤±æ•—: {e}", 'error')
        return False

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='ç·©å­˜ç®¡ç†å·¥å…·')
    parser.add_argument('--list', action='store_true', help='åˆ—å‡ºæ‰€æœ‰ç·©å­˜æ–‡ä»¶')
    parser.add_argument('--verbose', action='store_true', help='é¡¯ç¤ºè©³ç´°ä¿¡æ¯')
    parser.add_argument('--backup', action='store_true', help='å‚™ä»½ç·©å­˜æ–‡ä»¶')
    parser.add_argument('--backup-file', type=str, help='å‚™ä»½æŒ‡å®šçš„ç·©å­˜æ–‡ä»¶')
    parser.add_argument('--clean-old', type=int, metavar='DAYS', help='æ¸…ç†è¶…éæŒ‡å®šå¤©æ•¸çš„ç·©å­˜æ–‡ä»¶')
    parser.add_argument('--force-clean', action='store_true', help='å¼·åˆ¶æ¸…ç†ï¼ˆåŒ…æ‹¬é—œéµç·©å­˜ï¼‰')
    parser.add_argument('--check', action='store_true', help='é‹è¡Œç³»çµ±å¥åº·æª¢æŸ¥')
    parser.add_argument('--health', action='store_true', help='æª¢æŸ¥ç·©å­˜å¥åº·ç‹€æ…‹')
    parser.add_argument('--restore', action='store_true', help='å¾æœ€æ–°å‚™ä»½æ¢å¾©ç·©å­˜')
    parser.add_argument('--restore-backup', type=str, help='å¾æŒ‡å®šå‚™ä»½æ¢å¾©ç·©å­˜')
    parser.add_argument('--restore-file', type=str, help='æ¢å¾©æŒ‡å®šçš„æ–‡ä»¶')
    parser.add_argument('--cleanup-backups', action='store_true', help='æ¸…ç†éèˆŠçš„å‚™ä»½')
    parser.add_argument('--auto', action='store_true', help='åŸ·è¡Œè‡ªå‹•åŒ–ç¶­è­·ï¼ˆå‚™ä»½+æ¸…ç†ï¼‰')
    parser.add_argument('--init', action='store_true', help='åˆå§‹åŒ–ç·©å­˜ç³»çµ±')
    
    args = parser.parse_args()
    
    if args.init:
        init_cache()
    
    if args.list:
        list_cache(args.verbose)
    
    if args.backup:
        backup_cache()
    
    if args.backup_file:
        backup_single_file(args.backup_file)
    
    if args.clean_old is not None:
        clean_old_cache(args.clean_old, args.force_clean)
    
    if args.check:
        run_system_check()
    
    if args.health:
        print_cache_health()
    
    if args.restore:
        restore_backup()
    
    if args.restore_backup:
        if args.restore_file:
            restore_backup(args.restore_backup, args.restore_file)
        else:
            restore_backup(args.restore_backup)
    
    if args.cleanup_backups:
        auto_cleanup_backups()
    
    if args.auto:
        # åŸ·è¡Œè‡ªå‹•åŒ–ç¶­è­·
        print("\n=== åŸ·è¡Œè‡ªå‹•åŒ–ç·©å­˜ç¶­è­· ===")
        
        # 1. åŸ·è¡Œå¥åº·æª¢æŸ¥
        health_report = cache_health_check()
        print_cache_health(health_report)
        
        # 2. è‡ªå‹•å‚™ä»½éœ€è¦å‚™ä»½çš„æ–‡ä»¶
        automatic_backup()
        
        # 3. æ¸…ç†éèˆŠçš„ç·©å­˜
        for filename, config in CACHE_CONFIG.items():
            if config.get('auto_clean', True):
                file_path = os.path.join(CACHE_DIR, filename)
                if os.path.exists(file_path):
                    age_days = (time.time() - os.path.getmtime(file_path)) / (24 * 3600)
                    if age_days > config.get('retention_days', 7):
                        log_event(f"è‡ªå‹•æ¸…ç†éæœŸç·©å­˜: {filename}", 'info')
                        backup_single_file(filename)  # å…ˆå‚™ä»½
                        os.remove(file_path)  # å†åˆªé™¤
        
        # 4. æ¸…ç†éèˆŠçš„å‚™ä»½
        auto_cleanup_backups()
        
        print("\n=== è‡ªå‹•ç¶­è­·å®Œæˆ ===")
    
    # å¦‚æœæ²’æœ‰æä¾›ä»»ä½•åƒæ•¸ï¼Œé¡¯ç¤ºå¹«åŠ©
    if not any([args.list, args.backup, args.backup_file, args.clean_old is not None, 
                args.check, args.health, args.restore, args.restore_backup, 
                args.cleanup_backups, args.auto, args.init]):
        parser.print_help()
